# ğŸš¢ Titanic Survival Prediction  
### End-to-End Machine Learning Project  
*Kaggle Competition: [Titanic - Machine Learning from Disaster](https://www.kaggle.com/competitions/titanic)*

---

## ğŸ¯ Project Objective

Develop predictive models to estimate passenger survival probability using demographic and socio-economic features.

This project was structured as a **multi-stage experimental pipeline**, designed to evaluate the impact of:

- ğŸ“Š Data preprocessing
- ğŸ§  Feature engineering
- ğŸ¤– Model selection
- âš™ï¸ Algorithmic complexity
- ğŸ“‰ Overfitting risk

---

# ğŸ”¬ Methodology

The project was divided into four progressive development stages.

---

## ğŸ§± Stage 1 â€” Baseline Model

Established a performance benchmark with minimal preprocessing.

### âœ” Key Actions

- Exploratory Data Analysis using *ydata-profiling*
- Removal of high-cardinality features
- Missing value imputation:
  - Mean (numerical)
  - Mode (categorical)
- Removal of text-based features
- Models trained:
  - Decision Tree
  - K-Nearest Neighbors
  - Logistic Regression
- Evaluation metrics:
  - Accuracy
  - Confusion Matrix

**Public Kaggle Score:** `0.66746`

---

## ğŸ§© Stage 2 â€” Categorical Feature Engineering

Integrated categorical variables properly into the modeling pipeline.

### âœ” Improvements

- Custom transformations using `lambda`
- Encoding via *OneHotEncoder*
- Same algorithms maintained for controlled comparison

**Public Kaggle Score:** `0.76555`

ğŸ“ˆ Significant performance gain after proper categorical encoding.

---

## ğŸ§  Stage 3 â€” Advanced Feature Engineering & Optimization

Focused on domain understanding and feature enhancement.

### âœ” Enhancements

**Feature Scaling**
- Standardization of `Age` and `Fare`

**New Engineered Features**
- `FamilySize` = SibSp + Parch + 1
- `IsAlone` = Binary indicator

**Correlation Analysis**
- Selection of statistically relevant variables

Models maintained:
- Decision Tree
- KNN
- Logistic Regression

**Public Kaggle Score:** `0.77033` â­

ğŸ“ˆ Incremental improvement driven by feature engineering.

---

## ğŸ¤– Stage 4 â€” Advanced Algorithms

Tested more complex models while keeping all engineered features.

### âœ” Models Evaluated

- Logistic Regression
- Random Forest
- MLPClassifier (Neural Network)

Although the MLP achieved the highest validation accuracy, it underperformed in the Kaggle submission â€” indicating probable **overfitting**.

**Public Kaggle Score:** `0.69856`

âš  Clear evidence of reduced generalization.

---

# ğŸ“Š Performance Evolution

| Stage | Strategy | Public Score |
|-------|----------|-------------|
| 1 | Baseline | 0.66746 |
| 2 | Categorical Encoding | 0.76555 |
| 3 | Feature Engineering | **0.77033** |
| 4 | Complex Models | 0.69856 |

---

# ğŸ§  Key Takeaways

- Feature engineering had greater impact than model complexity.
- More complex models do not guarantee better generalization.
- Validation performance must be interpreted cautiously.
- Structured experimentation improves model development clarity.

---

# ğŸ›  Tech Stack

- Python
- Pandas
- NumPy
- Scikit-Learn
- Matplotlib
- Seaborn
- ydata-profiling

---

# ğŸ“Œ Next Steps

- Implement robust cross-validation strategy
- Apply GridSearch / RandomSearch
- Experiment with Gradient Boosting (XGBoost, LightGBM)
- Improve feature selection pipeline

---

ğŸ“¬ Feel free to connect or reach out for collaboration.
